{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/%E5%AF%B9%E8%AF%9D%E6%8A%BD%E5%8F%96_%E4%BD%BF%E7%94%A8colab%E8%BF%9B%E8%A1%8Cfinetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zElyM_0tGpZH",
        "outputId": "accd57fa-0c9e-4490-dd19-33c49e261bcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade huggingface_hub\n",
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjFuGKxUFvbY",
        "outputId": "6a7f31e5-9972-4b60-e295-5e3cec59f7d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 6750, done.\u001b[K\n",
            "remote: Counting objects: 100% (769/769), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 6750 (delta 506), reused 661 (delta 451), pack-reused 5981\u001b[K\n",
            "Receiving objects: 100% (6750/6750), 204.48 MiB | 20.83 MiB/s, done.\n",
            "Resolving deltas: 100% (4857/4857), done.\n",
            "Updating files: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAcYKc9qGwcY",
        "outputId": "6782f84c-bfef-4a4c-dd78-73df2880ffb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd LLaMA-Factory\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install -q einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OSdjnmbHCSz"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIhsKveEIT7C",
        "outputId": "d8a401ce-d594-4ce9-a380-2fd18c511569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/LLaMA-Factory/dataset': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls /content/LLaMA-Factory/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFANKTp0JWSZ",
        "outputId": "ac06bc64-d529-4c0c-d23a-050eb4443cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfrKNsHhJbKC",
        "outputId": "197be821-47db-4b0b-e2b0-79126b9a0a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CardBuild/HaruhiZero/chinese_dialogue_clean.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/dialogue_extract.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/dialogue_negative.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/english_dialogue_clean.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/english_dialogue_extract.jsonl\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/CardBuild/HaruhiZero/*dialogue*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYOUoHDK4x9"
      },
      "source": [
        "\n",
        "è¯·ä¸ºæˆ‘å®ç°ä¸€æ®µpythonä»£ç ï¼ŒæŠŠinput_pathä¸‹çš„å‡ ä¸ªfile_nameså¯¹åº”çš„jsonlæ–‡ä»¶(utf-8, ensure_ascii=False) è¿›è¡Œè¯»å–\n",
        "\n",
        "ç„¶åä»¥jsonæ ¼å¼å†™å…¥target_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä¸‰ä¸ªæ–‡ä»¶åˆ†åˆ«åœ¨\n",
        "\n",
        "æ¸…æ´åçš„ä¸­æ–‡å¯¹è¯\n",
        "https://drive.google.com/file/d/1Oc3R5SCiVXL4mwKBx1Zh0PS9Fv6YeYos/view?usp=sharing\n",
        "\n",
        "å¯¹è¯æŠ½å–-è´Ÿæ ·ä¾‹\n",
        "https://drive.google.com/file/d/1u4HuXgI7Ava8HIf_L57x1HRQygZ6_tiX/view?usp=sharing\n",
        "\n",
        "æ¸…æ´åçš„è‹±æ–‡å¯¹è¯\n",
        "https://drive.google.com/file/d/1--iJ_sqiECjwtcdLkbiAmvyP_RodTl4d/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "cxEZBZDJWR6l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYZqDgizJdyy"
      },
      "outputs": [],
      "source": [
        "input_path = \"/content/drive/MyDrive/CardBuild/HaruhiZero/\"\n",
        "file_names = [\"chinese_dialogue_clean.jsonl\",\"dialogue_negative.jsonl\",\"english_dialogue_clean.jsonl\"]\n",
        "# file_names = [\"chinese_dialogue_clean.jsonl\"]\n",
        "target_path = \"/content/LLaMA-Factory/data/\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "target_file = \"/content/LLaMA-Factory/data/dialogue_merged.json\"\n",
        "\n",
        "datas = []\n",
        "for file_name in file_names:\n",
        "    with open(os.path.join(input_path, file_name), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            # delete the key except system and conversations\n",
        "            if \"source\" in data:\n",
        "                del data[\"source\"]\n",
        "            datas.append(data)\n",
        "\n",
        "with open(target_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "target_file_jsonl = \"/content/LLaMA-Factory/data/dialogue_merged.jsonl\"\n",
        "\n",
        "\n",
        "with open(target_file_jsonl, 'w', encoding='utf-8') as f:\n",
        "    for data in datas:\n",
        "        f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVFtJx-aMk9C"
      },
      "outputs": [],
      "source": [
        "for data in datas:\n",
        "    if \"system\" not in data:\n",
        "        print(\"system not in data\")\n",
        "    elif \"conversations\" not in data:\n",
        "        print(\"conversations not in data\")\n",
        "    elif len(data[\"conversations\"]) != 2:\n",
        "        print(\"conversations is empty\")\n",
        "    else:\n",
        "        text = data[\"conversations\"][0]\n",
        "        continue\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJnK1Za2J-0A"
      },
      "source": [
        "è¯·ä¸ºæˆ‘å®ç°ä¸€æ®µpythonä»£ç ï¼Œå¯¹äº/content/LLaMA-Factory/data/dataset_info.json å¤‡ä»½åˆ°dataset_info_back.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exqKxSYMLKMf",
        "outputId": "04679ff5-0e13-4af7-afc6-699d13eb8ab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backup of /content/LLaMA-Factory/data/dataset_info.json to /content/LLaMA-Factory/data/dataset_info_back.json succeeded!\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "original_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "backup_file = '/content/LLaMA-Factory/data/dataset_info_back.json'\n",
        "\n",
        "# Make a copy of the original file\n",
        "shutil.copy(original_file, backup_file)\n",
        "\n",
        "# Verify copy succeeded\n",
        "if os.path.exists(backup_file):\n",
        "    print(f'Backup of {original_file} to {backup_file} succeeded!')\n",
        "else:\n",
        "    print(f'Backup of {original_file} to {backup_file} failed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48s-ojJQLOsZ"
      },
      "source": [
        "ä¸ºæˆ‘å®ç°ä¸€æ®µpythonä»£ç ï¼Œå¤åˆ¶/content/LLaMA-Factory/data/dataset_info.jsonä¸­çš„ä¿¡æ¯\n",
        "\n",
        "å†™å…¥åˆ° /content/LLaMA-Factory/data/dataset_info.json\n",
        "\n",
        "å¹¶ä¸”åœ¨æœ«å°¾æ·»åŠ \n",
        "\n",
        "\n",
        "```json\n",
        "\"haruhi_dialogue_extract\": {\n",
        "  \"file_name\": \"dialogue_merged.json\",\n",
        "  \"columns\": {\n",
        "    \"messages\": \"conversations\",\n",
        "    \"system\": \"system\",\n",
        "    \"tools\": \"tools\"\n",
        "  },\n",
        "  \"tags\": {\n",
        "    \"role_tag\": \"from\",\n",
        "    \"content_tag\": \"value\"\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ik4Bvq1LNpE",
        "outputId": "221c1019-0a28-4f8e-ab9d-7464bd4d507c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data copied and new info appended to /content/LLaMA-Factory/data/dataset_info.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "original_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "new_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "\n",
        "with open(original_file, 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "data['haruhi_dialogue_extract'] = {\n",
        "  \"file_name\": \"/content/LLaMA-Factory/data/dialogue_merged.jsonl\",\n",
        "  \"formatting\":\"sharegpt\",\n",
        "  \"columns\": {\n",
        "    \"messages\": \"conversations\",\n",
        "    \"system\": \"system\",\n",
        "  },\n",
        "  \"tags\": {\n",
        "    \"role_tag\": \"from\",\n",
        "    \"content_tag\": \"value\",\n",
        "    \"user_tag\": \"human\",\n",
        "    \"assistant_tag\":\"gpt\",\n",
        "  }\n",
        "}\n",
        "\n",
        "with open(new_file, 'w') as f:\n",
        "  json.dump(data, f, indent=2)\n",
        "\n",
        "print('Data copied and new info appended to', new_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaruMxYdL6QU",
        "outputId": "74872790-5baf-4c81-c2af-5bd6c39d1b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tiktoken transformers_stream_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqZswBFMN6-C"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhZPqKBcGHau",
        "outputId": "0a4999c8-4ebe-4ff3-9fa5-6f663481785c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-01-25 01:49:26.076683: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-25 01:49:26.076737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-25 01:49:26.078081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-25 01:49:27.281205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/25/2024 01:49:29 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1828] 2024-01-25 01:49:29,415 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "01/25/2024 01:49:29 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "01/25/2024 01:49:29 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=qwen_1_8-finetuned/runs/Jan25_01-49-29_29f45e3f8461,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=qwen_1_8-finetuned,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=qwen_1_8-finetuned,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/qwen.tiktoken\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file tokenizer.json from cache at None\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 01:49:30,468 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 01:49:30,653 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-01-25 01:49:30,654 >> Model config QWenConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"fp16\": false,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"transformers_version\": \"4.37.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3478] 2024-01-25 01:49:30,781 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1428] 2024-01-25 01:49:30,782 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 01:49:30,783 >> Generate config GenerationConfig {}\n",
            "\n",
            "Try importing flash-attention for faster inference...\n",
            "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.73s/it]\n",
            "[INFO|modeling_utils.py:4352] 2024-01-25 01:49:36,597 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4360] 2024-01-25 01:49:36,597 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-1_8B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
            "[INFO|configuration_utils.py:781] 2024-01-25 01:49:36,687 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/generation_config.json\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 01:49:36,688 >> Generate config GenerationConfig {\n",
            "  \"chat_format\": \"chatml\",\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 512,\n",
            "  \"max_window_size\": 6144,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"top_k\": 0,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[WARNING|modeling_utils.py:2134] 2024-01-25 01:49:36,689 >> You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.model.loader - trainable params: 3145728 || all params: 1839974400 || trainable%: 0.1710\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.data.template - Replace eos token: <|im_end|>\n",
            "01/25/2024 01:49:36 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
            "Using custom data configuration default-7b563057c8b8ec89\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Converting format of dataset:   0% 0/72927 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eebdf306e7652ea7.arrow\n",
            "Converting format of dataset: 100% 72927/72927 [00:04<00:00, 17624.58 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/72927 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43b5017f793f2e0d.arrow\n",
            "Running tokenizer on dataset: 100% 72927/72927 [02:57<00:00, 411.13 examples/s]\n",
            "input_ids:\n",
            "[151644, 8948, 198, 89012, 22382, 1355, 14311, 3837, 118797, 90919, 9370, 105051, 90395, 66017, 17714, 2236, 68805, 271, 10061, 594, 1744, 432, 3019, 553, 3019, 198, 16, 13, 62079, 1946, 14311, 1119, 17432, 3561, 3837, 105653, 18493, 1708, 44931, 198, 17, 13, 96155, 121, 18158, 104588, 99700, 105051, 104597, 21276, 3837, 104317, 104588, 100908, 9370, 104283, 17340, 1053, 553, 11, 53497, 246, 99871, 18493, 443, 72995, 15946, 151645, 198, 151644, 872, 198, 100039, 44729, 30858, 101882, 23990, 102482, 105444, 101056, 106183, 100363, 3837, 101889, 101882, 115083, 104808, 110878, 3837, 103941, 105705, 77540, 69442, 115978, 3837, 114808, 101888, 75437, 75437, 99733, 99733, 99424, 50404, 9370, 105051, 1773, 100039, 44729, 30858, 103941, 23031, 105705, 77540, 102565, 99938, 17714, 104629, 102401, 3837, 101882, 115083, 28951, 25710, 8997, 2073, 103933, 35946, 99405, 100146, 56568, 100854, 99938, 3837, 35946, 100684, 101482, 3837, 99519, 104806, 35946, 105715, 99938, 100363, 3837, 104662, 36993, 105214, 99677, 47534, 3837, 99190, 104355, 102410, 104789, 99899, 106800, 854, 100039, 44729, 30858, 44793, 8997, 115083, 100307, 103546, 2073, 43288, 102095, 106800, 3837, 56568, 106374, 100623, 99190, 104355, 115978, 99212, 101258, 104483, 106800, 3837, 113444, 100018, 101315, 101315, 101037, 88774, 106283, 3837, 116315, 101920, 99364, 99476, 107008, 8997, 100039, 44729, 30858, 102837, 73145, 57452, 115083, 3837, 107038, 104461, 28072, 102300, 106983, 23990, 102482, 3837, 118711, 9370, 118724, 102764, 34204, 77144, 20742, 2073, 101045, 3837, 103945, 101051, 75108, 99414, 99385, 104339, 107446, 3837, 99212, 102290, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 3837, 98641, 110359, 101099, 69249, 102290, 88774, 2073, 56568, 100245, 854, 100369, 18600, 99694, 39426, 103427, 8997, 103969, 71134, 99190, 100039, 44729, 30858, 99476, 103206, 2073, 35946, 109916, 104447, 88774, 102068, 3837, 42411, 102068, 105958, 104447, 9370, 3837, 23990, 102482, 57218, 42411, 65676, 99396, 65676, 99535, 3837, 100002, 42411, 99883, 3837, 102347, 107516, 102688, 104970, 75108, 99414, 99385, 101102, 9370, 102011, 104606, 3837, 42411, 100678, 104447, 11319, 42411, 102347, 99486, 99730, 99654, 100854, 8997, 68536, 42411, 99786, 105745, 100006, 118433, 26939, 100648, 47606, 3837, 23031, 99283, 104718, 100531, 35727, 107690, 3837, 42411, 102130, 100006, 33108, 42411, 14777, 99191, 3837, 30440, 14777, 116244, 100363, 3837, 105799, 112356, 36993, 11622, 75108, 99414, 99385, 3837, 101982, 100710, 42411, 102196, 9370, 105288, 1773, 60894, 102157, 100006, 105921, 108165, 118433, 100623, 96050, 117093, 104461, 99670, 106262, 3837, 101885, 104501, 99650, 102747, 102323, 3837, 99947, 99541, 106262, 3837, 99517, 99509, 9370, 106742, 107202, 3837, 99670, 20412, 23990, 102482, 3837, 100136, 42411, 104871, 107169, 99733, 100039, 44729, 30858, 8997, 99999, 3837, 99246, 101373, 99916, 99998, 49828, 30534, 99904, 42411, 100638, 8997, 115083, 9370, 112948, 106823, 121967, 71618, 3837, 100155, 101192, 90663, 44793, 2073, 102068, 3837, 56568, 102068, 105958, 104447, 9370, 3837, 105365, 108089, 99411, 39426, 36587, 36587, 108809, 3837, 105622, 99260, 88051, 88774, 100039, 44729, 30858, 104203, 49238, 23990, 102482, 108231, 3837, 104203, 104094, 2073, 105509, 56568, 113955, 57443, 70927, 107484, 100363, 3837, 73670, 100640, 36587, 3837, 35946, 104047, 99319, 101413, 99319, 49187, 9370, 1773, 100009, 104515, 104211, 56568, 100380, 3837, 105665, 49828, 99236, 99234, 3837, 35946, 101413, 49828, 99236, 99234, 3837, 42411, 103939, 82647, 101883, 104573, 1773, 100155, 105665, 49828, 99843, 34187, 3837, 99212, 35946, 112639, 106162, 101149, 3837, 42411, 103977, 42140, 101883, 104573, 88774, 115083, 48934, 102210, 44793, 2073, 109111, 16530, 59879, 108792, 20221, 99327, 88774, 100039, 44729, 30858, 106823, 107319, 3837, 102135, 99461, 105962, 99212, 101492, 99612, 113988, 105313, 120011, 63109, 9370, 102188, 81812, 3837, 101077, 111798, 1773, 151645, 198, 151644, 77091, 198, 4913, 1708, 788, 6523, 40666, 237, 44729, 30858, 101882, 115083, 104808, 110878, 90395, 23031, 105705, 77540, 69442, 99938, 17714, 104629, 102401, 3837, 101882, 115083, 28951, 25710, 1773, 114808, 101888, 75437, 75437, 99733, 99733, 99424, 50404, 9370, 105051, 1773, 106283, 101197, 104432, 3837, 100039, 44729, 30858, 104643, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 104339, 3837, 114808, 115083, 9370, 106319, 33108, 117978, 1773, 497, 330, 443, 72995, 788, 61753, 11817, 361, 788, 330, 103933, 35946, 99405, 100146, 56568, 100854, 99938, 3837, 35946, 100684, 101482, 3837, 99519, 104806, 35946, 105715, 99938, 100363, 3837, 104662, 36993, 105214, 99677, 47534, 3837, 99190, 104355, 102410, 104789, 99899, 106800, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 43288, 102095, 106800, 3837, 56568, 106374, 100623, 99190, 104355, 115978, 99212, 101258, 104483, 106800, 3837, 113444, 100018, 101315, 101315, 101037, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 101045, 3837, 103945, 101051, 75108, 99414, 99385, 104339, 107446, 3837, 99212, 102290, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 3837, 98641, 110359, 101099, 69249, 102290, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 56568, 100245, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 35946, 109916, 104447, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 102068, 3837, 56568, 102068, 105958, 104447, 9370, 3837, 105365, 108089, 99411, 39426, 36587, 36587, 108809, 3837, 105622, 99260, 88051, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 105509, 56568, 113955, 57443, 70927, 107484, 100363, 3837, 35946, 104047, 99319, 101413, 99319, 49187, 9370, 1773, 100009, 104515, 104211, 56568, 100380, 3837, 105665, 49828, 99236, 99234, 3837, 35946, 101413, 49828, 99236, 99234, 3837, 42411, 103939, 82647, 101883, 104573, 1773, 100155, 105665, 49828, 99843, 34187, 3837, 99212, 35946, 112639, 106162, 101149, 3837, 42411, 103977, 42140, 101883, 104573, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 109111, 16530, 59879, 108792, 20221, 99327, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 101241, 99821, 12857, 74386, 3837, 101451, 99807, 45629, 26288, 102844, 3837, 35946, 104624, 100012, 105572, 99464, 99164, 99172, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 99494, 34187, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 9207, 13989, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "ç»™å®šinput paragraphï¼ŒæŠ½å–å…¶ä¸­çš„å¯¹è¯ï¼Œå¹¶è¾“å‡ºä¸ºjsonæ ¼å¼\n",
            "\n",
            "Let's think it step by step\n",
            "1. summarize input paragraph into bullet formatï¼Œå­˜å‚¨åœ¨summaryå­—æ®µ\n",
            "2. æŠ½å–æ¯ä¸€å¥å¯¹è¯çš„å†…å®¹ dialogueï¼Œåˆ¤æ–­æ¯ä¸€å¥è¯çš„è¯´è¯äºº said by, å­˜å‚¨åœ¨conversationsä¸­<|im_end|>\n",
            "<|im_start|>user\n",
            "å¤å­æ˜è¦æ±‚å•ç¿”é‡å¤ä¹‹å‰è¯´è¿‡çš„è¯ï¼Œç„¶åè¦æ±‚é»æ˜ç»™ä»–åšé¥­ï¼Œæœ€ç»ˆåƒäº†ä¸¤ä»½é¥­èœï¼Œå¼•å‘äº†å…³äºæ‰“æ‰“æ€æ€ç”Ÿæ´»é€‰æ‹©çš„å¯¹è¯ã€‚å¤å­æ˜æœ€ç»ˆä»¥åƒäº†ä¸¤ç¢—é¥­ä¸ºèƒœåˆ©ç†ç”±ï¼Œè¦æ±‚é»æ˜è®¤è¾“ã€‚\n",
            "â€œå¯æ˜¯æˆ‘åƒçš„æ˜¯ä½ åšçš„é¥­ï¼Œæˆ‘å¹¶ä¸äºï¼Œå› ä¸ºè¦æ˜¯æˆ‘å»åšé¥­çš„è¯ï¼Œä¸ä½†ä¼šæµªè´¹çµåŠ›ï¼Œåšå‡ºæ¥çš„æœ‰å¯èƒ½è¿˜æ²¡è¿™ä¹ˆç¾å‘³â€å¤å­æ˜é“ã€‚\n",
            "é»æ˜æ‘‡æ‘‡å¤´â€œè¿™å¹¶ä¸æ˜¯ç¾å‘³ï¼Œä½ èº«åçš„äººåšå‡ºæ¥çš„é¥­èœé‚£æ‰æ˜¯çœŸæ­£çš„ç¾å‘³ï¼Œæœ‰å…´è¶£ä¸€èµ·å°å°å—â€\n",
            "æ°”æ°›ï¼Œåˆ¹é‚£ä¹‹é—´ä¾¿å†·äº†ä¸‹æ¥ã€‚\n",
            "å¤å­æ˜ç›®å…‰ç›´è§†é»æ˜ï¼Œä¸€æ‰‹å´æ˜¯æèµ·äº†èº«è¾¹çš„å•ç¿”ï¼Œè¡€è…¥çš„ç¬‘æ„æº¢äºè¨€è¡¨â€œå…¶å®ï¼Œæƒ³è¦å¾—åˆ°äº”å½©çŸ³çš„æ–¹æ³•è¿˜æœ‰ä¸€ä¸ªï¼Œé‚£ä¾¿æ˜¯ç›´æ¥å°†ä¸ƒçªç²ç‘å¿ƒæå‡ºæ¥ï¼Œè£…åœ¨æˆ‘çš„èº«ä½“é‡Œä¾¿æ˜¯â€\n",
            "â€œä½ æ•¢â€ä¸¤ä¸ªå­—è„±å£è€Œå‡ºã€‚\n",
            "è¿™æ¬¡æ¢åšå¤å­æ˜å†·ç¬‘äº†â€œæˆ‘æœ‰ä½•ä¸æ•¢â€\n",
            "ç¡®å®ï¼Œä»–ç¡®å®æ²¡ä»€ä¹ˆä¸æ•¢çš„ï¼Œå•ç¿”ä¸ä»–éäº²éæ•…ï¼Œå¯¹äºä»–æ¥è¯´ï¼Œæœ¬æ¥å°±æ˜¯ä¸€ä¸ªç”¨æ¥æ¿€å‘äº”å½©çŸ³åŠ›é‡çš„å·¥å…·è€Œå·²ï¼Œä»–ä¸ºä»€ä¹ˆä¸æ•¢ï¼Ÿä»–æœ¬æ¥å°±æ˜¯åº”è¯¥è¿™æ ·åšçš„ã€‚\n",
            "è€Œä»–å´æ²¡æœ‰ä»€ä¹ˆèƒ½å¤Ÿå¨æ…‘åˆ°ä»–çš„å­˜åœ¨ï¼Œä»¥è‡ªå·±ç°åœ¨çš„é€†å¤©ä¹‹åŠ›ï¼Œä»–æˆ–è®¸èƒ½å¤Ÿå’Œä»–ä¸€æˆ˜ï¼Œå¯ä¸€å¼€æˆ˜çš„è¯ï¼Œä»–å°±åŠ¿å¿…ä¼šç”¨äº”å½©çŸ³ï¼Œä»è€ŒåŠ å¿«ä»–åŠ¨ä½œçš„èŠ‚å¥ã€‚ç„¶å”¯ä¸€èƒ½å¤Ÿå¯¹ä»–å½¢æˆçš„å¨æ…‘çš„äººï¼Œåœ¨è¿™ä¸€åˆ»å´æ˜¯ä¸ä¼šå‡ºæ‰‹ï¼Œè€Œä¸”å°±ç®—ä»–ä»¬é™·å…¥å±æœºï¼Œæ²‰é™å‡ºæ‰‹ï¼Œå¥¹æ•‘çš„åªä¼šæ˜¯ä»–ï¼Œä¸ä¼šæ˜¯å•ç¿”ï¼Œä¸”ä»–ä¹Ÿä¸ä¼šåŠ¨æ‰‹æ€å¤å­æ˜ã€‚\n",
            "æ‰€ä»¥ï¼Œçœ¼ä¸‹çš„äº‹æƒ…è¿˜æ˜¯å¾—è¦é ä»–è§£å†³ã€‚\n",
            "é»æ˜çš„çœ‰å¤´å¾®å¾®è¹™èµ·ï¼Œè‹¥æœ‰æ‰€æ€é“â€œç¡®å®ï¼Œä½ ç¡®å®æ²¡ä»€ä¹ˆä¸æ•¢çš„ï¼Œä½ å°±å½“æˆ‘éšå£è¯´è¯´ç½¢äº†ï¼Œä¸å¿…è¾ƒçœŸâ€\n",
            "å¤å­æ˜ä¸€è¾¹è§£å•ç¿”çš„è¡£æœï¼Œä¸€è¾¹è¯´é“â€œè‹¥æ˜¯ä½ è¿˜æœ‰ä»€ä¹ˆè¯æ²¡è¯´å®Œçš„è¯ï¼Œå¯ä»¥ç»§ç»­è¯´ï¼Œæˆ‘ä¹Ÿå¯ä»¥è¾¹æŒ–è¾¹å¬çš„ã€‚åªæ˜¯æˆ‘è¦æé†’ä½ ä¸€ç‚¹ï¼Œä½ è¯´å¾—è¶Šå¿«ï¼Œæˆ‘æŒ–å¾—è¶Šå¿«ï¼Œä»–å°±ä¼šå°‘ä¸€äº›ç—›è‹¦ã€‚è‹¥ä½ è¯´å¾—æ…¢äº†ï¼Œé‚£æˆ‘ä¾¿ä¼šæ”¾ä¸‹é€Ÿåº¦ï¼Œä»–ä¹Ÿä¼šå¤šä¸€äº›ç—›è‹¦â€\n",
            "é»æ˜å¾®æ€’é“â€œä½ æ€ä¹ˆä¸æŒ‰å¥—è·¯å‡ºç‰Œâ€\n",
            "å¤å­æ˜å¾®å¾®ä¸€ç¬‘ï¼Œçœ‹æ¥å·²ç»æ‰¾åˆ°äº†é‚£é¢—ä¸ƒçªç²ç‘å¿ƒçš„å‡†ç¡®ä½ç½®ï¼Œå‡†å¤‡ä¸‹æ‰‹ã€‚<|im_end|>\n",
            "<|im_start|>assistant\n",
            "{\"summary\": \"- å¤å­æ˜è¦æ±‚é»æ˜ç»™ä»–åšé¥­ï¼Œå¹¶ä»¥åƒäº†ä¸¤ä»½é¥­ä¸ºèƒœåˆ©ç†ç”±ï¼Œè¦æ±‚é»æ˜è®¤è¾“ã€‚å¼•å‘äº†å…³äºæ‰“æ‰“æ€æ€ç”Ÿæ´»é€‰æ‹©çš„å¯¹è¯ã€‚æ°”æ°›å˜å¾—ç´§å¼ ï¼Œå¤å­æ˜æå‡ºäº†ç›´æ¥å°†ä¸ƒçªç²ç‘å¿ƒæå‡ºæ¥çš„æ–¹æ³•ï¼Œå¼•å‘äº†é»æ˜çš„è­¦å‘Šå’Œåé©³ã€‚\", \"conversations\": [{\"dialogue\": \"å¯æ˜¯æˆ‘åƒçš„æ˜¯ä½ åšçš„é¥­ï¼Œæˆ‘å¹¶ä¸äºï¼Œå› ä¸ºè¦æ˜¯æˆ‘å»åšé¥­çš„è¯ï¼Œä¸ä½†ä¼šæµªè´¹çµåŠ›ï¼Œåšå‡ºæ¥çš„æœ‰å¯èƒ½è¿˜æ²¡è¿™ä¹ˆç¾å‘³\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"è¿™å¹¶ä¸æ˜¯ç¾å‘³ï¼Œä½ èº«åçš„äººåšå‡ºæ¥çš„é¥­èœé‚£æ‰æ˜¯çœŸæ­£çš„ç¾å‘³ï¼Œæœ‰å…´è¶£ä¸€èµ·å°å°å—\", \"said_by\": \"é»æ˜\"}, {\"dialogue\": \"å…¶å®ï¼Œæƒ³è¦å¾—åˆ°äº”å½©çŸ³çš„æ–¹æ³•è¿˜æœ‰ä¸€ä¸ªï¼Œé‚£ä¾¿æ˜¯ç›´æ¥å°†ä¸ƒçªç²ç‘å¿ƒæå‡ºæ¥ï¼Œè£…åœ¨æˆ‘çš„èº«ä½“é‡Œä¾¿æ˜¯\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"ä½ æ•¢\", \"said_by\": \"é»æ˜\"}, {\"dialogue\": \"æˆ‘æœ‰ä½•ä¸æ•¢\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"ç¡®å®ï¼Œä½ ç¡®å®æ²¡ä»€ä¹ˆä¸æ•¢çš„ï¼Œä½ å°±å½“æˆ‘éšå£è¯´è¯´ç½¢äº†ï¼Œä¸å¿…è¾ƒçœŸ\", \"said_by\": \"é»æ˜\"}, {\"dialogue\": \"è‹¥æ˜¯ä½ è¿˜æœ‰ä»€ä¹ˆè¯æ²¡è¯´å®Œçš„è¯ï¼Œæˆ‘ä¹Ÿå¯ä»¥è¾¹æŒ–è¾¹å¬çš„ã€‚åªæ˜¯æˆ‘è¦æé†’ä½ ä¸€ç‚¹ï¼Œä½ è¯´å¾—è¶Šå¿«ï¼Œæˆ‘æŒ–å¾—è¶Šå¿«ï¼Œä»–å°±ä¼šå°‘ä¸€äº›ç—›è‹¦ã€‚è‹¥ä½ è¯´å¾—æ…¢äº†ï¼Œé‚£æˆ‘ä¾¿ä¼šæ”¾ä¸‹é€Ÿåº¦ï¼Œä»–ä¹Ÿä¼šå¤šä¸€äº›ç—›è‹¦\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"ä½ æ€ä¹ˆä¸æŒ‰å¥—è·¯å‡ºç‰Œ\", \"said_by\": \"é»æ˜\"}, {\"dialogue\": \"å¢¨å®ˆæˆè§„ï¼Œä¹ƒå…µå®¶å¤§å¿Œï¼Œæˆ‘è¿™ä¹Ÿæ˜¯ä¸ºäº†è‡ªèº«çš„å®‰å…¨ç€æƒ³\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"æ€ä¹ˆäº†\", \"said_by\": \"å¤å­æ˜\"}]}<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 1708, 788, 6523, 40666, 237, 44729, 30858, 101882, 115083, 104808, 110878, 90395, 23031, 105705, 77540, 69442, 99938, 17714, 104629, 102401, 3837, 101882, 115083, 28951, 25710, 1773, 114808, 101888, 75437, 75437, 99733, 99733, 99424, 50404, 9370, 105051, 1773, 106283, 101197, 104432, 3837, 100039, 44729, 30858, 104643, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 104339, 3837, 114808, 115083, 9370, 106319, 33108, 117978, 1773, 497, 330, 443, 72995, 788, 61753, 11817, 361, 788, 330, 103933, 35946, 99405, 100146, 56568, 100854, 99938, 3837, 35946, 100684, 101482, 3837, 99519, 104806, 35946, 105715, 99938, 100363, 3837, 104662, 36993, 105214, 99677, 47534, 3837, 99190, 104355, 102410, 104789, 99899, 106800, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 43288, 102095, 106800, 3837, 56568, 106374, 100623, 99190, 104355, 115978, 99212, 101258, 104483, 106800, 3837, 113444, 100018, 101315, 101315, 101037, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 101045, 3837, 103945, 101051, 75108, 99414, 99385, 104339, 107446, 3837, 99212, 102290, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 3837, 98641, 110359, 101099, 69249, 102290, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 56568, 100245, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 35946, 109916, 104447, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 102068, 3837, 56568, 102068, 105958, 104447, 9370, 3837, 105365, 108089, 99411, 39426, 36587, 36587, 108809, 3837, 105622, 99260, 88051, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 105509, 56568, 113955, 57443, 70927, 107484, 100363, 3837, 35946, 104047, 99319, 101413, 99319, 49187, 9370, 1773, 100009, 104515, 104211, 56568, 100380, 3837, 105665, 49828, 99236, 99234, 3837, 35946, 101413, 49828, 99236, 99234, 3837, 42411, 103939, 82647, 101883, 104573, 1773, 100155, 105665, 49828, 99843, 34187, 3837, 99212, 35946, 112639, 106162, 101149, 3837, 42411, 103977, 42140, 101883, 104573, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 109111, 16530, 59879, 108792, 20221, 99327, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 101241, 99821, 12857, 74386, 3837, 101451, 99807, 45629, 26288, 102844, 3837, 35946, 104624, 100012, 105572, 99464, 99164, 99172, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 99494, 34187, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 9207, 13989, 151645]\n",
            "labels:\n",
            "{\"summary\": \"- å¤å­æ˜è¦æ±‚é»æ˜ç»™ä»–åšé¥­ï¼Œå¹¶ä»¥åƒäº†ä¸¤ä»½é¥­ä¸ºèƒœåˆ©ç†ç”±ï¼Œè¦æ±‚é»æ˜è®¤è¾“ã€‚å¼•å‘äº†å…³äºæ‰“æ‰“æ€æ€ç”Ÿæ´»é€‰æ‹©çš„å¯¹è¯ã€‚æ°”æ°›å˜å¾—ç´§å¼ ï¼Œå¤å­æ˜æå‡ºäº†ç›´æ¥å°†ä¸ƒçªç²ç‘å¿ƒæå‡ºæ¥çš„æ–¹æ³•ï¼Œå¼•å‘äº†é»æ˜çš„è­¦å‘Šå’Œåé©³ã€‚\", \"conversations\": [{\"dialogue\": \"å¯æ˜¯æˆ‘åƒçš„æ˜¯ä½ åšçš„é¥­ï¼Œæˆ‘å¹¶ä¸äºï¼Œå› ä¸ºè¦æ˜¯æˆ‘å»åšé¥­çš„è¯ï¼Œä¸ä½†ä¼šæµªè´¹çµåŠ›ï¼Œåšå‡ºæ¥çš„æœ‰å¯èƒ½è¿˜æ²¡è¿™ä¹ˆç¾å‘³\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"è¿™å¹¶ä¸æ˜¯ç¾å‘³ï¼Œä½ èº«åçš„äººåšå‡ºæ¥çš„é¥­èœé‚£æ‰æ˜¯çœŸæ­£çš„ç¾å‘³ï¼Œæœ‰å…´è¶£ä¸€èµ·å°å°å—\", \"said_by\": \"é»æ˜\"}, {\"dialogue\": \"å…¶å®ï¼Œæƒ³è¦å¾—åˆ°äº”å½©çŸ³çš„æ–¹æ³•è¿˜æœ‰ä¸€ä¸ªï¼Œé‚£ä¾¿æ˜¯ç›´æ¥å°†ä¸ƒçªç²ç‘å¿ƒæå‡ºæ¥ï¼Œè£…åœ¨æˆ‘çš„èº«ä½“é‡Œä¾¿æ˜¯\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"ä½ æ•¢\", \"said_by\": \"é»æ˜\"}, {\"dialogue\": \"æˆ‘æœ‰ä½•ä¸æ•¢\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"ç¡®å®ï¼Œä½ ç¡®å®æ²¡ä»€ä¹ˆä¸æ•¢çš„ï¼Œä½ å°±å½“æˆ‘éšå£è¯´è¯´ç½¢äº†ï¼Œä¸å¿…è¾ƒçœŸ\", \"said_by\": \"é»æ˜\"}, {\"dialogue\": \"è‹¥æ˜¯ä½ è¿˜æœ‰ä»€ä¹ˆè¯æ²¡è¯´å®Œçš„è¯ï¼Œæˆ‘ä¹Ÿå¯ä»¥è¾¹æŒ–è¾¹å¬çš„ã€‚åªæ˜¯æˆ‘è¦æé†’ä½ ä¸€ç‚¹ï¼Œä½ è¯´å¾—è¶Šå¿«ï¼Œæˆ‘æŒ–å¾—è¶Šå¿«ï¼Œä»–å°±ä¼šå°‘ä¸€äº›ç—›è‹¦ã€‚è‹¥ä½ è¯´å¾—æ…¢äº†ï¼Œé‚£æˆ‘ä¾¿ä¼šæ”¾ä¸‹é€Ÿåº¦ï¼Œä»–ä¹Ÿä¼šå¤šä¸€äº›ç—›è‹¦\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"ä½ æ€ä¹ˆä¸æŒ‰å¥—è·¯å‡ºç‰Œ\", \"said_by\": \"é»æ˜\"}, {\"dialogue\": \"å¢¨å®ˆæˆè§„ï¼Œä¹ƒå…µå®¶å¤§å¿Œï¼Œæˆ‘è¿™ä¹Ÿæ˜¯ä¸ºäº†è‡ªèº«çš„å®‰å…¨ç€æƒ³\", \"said_by\": \"å¤å­æ˜\"}, {\"dialogue\": \"æ€ä¹ˆäº†\", \"said_by\": \"å¤å­æ˜\"}]}<|im_end|>\n",
            "[INFO|training_args.py:1828] 2024-01-25 01:52:41,236 >> PyTorch: setting up devices\n",
            "[INFO|trainer.py:571] 2024-01-25 01:52:42,631 >> Using auto half precision backend\n",
            "[INFO|trainer.py:1721] 2024-01-25 01:52:42,948 >> ***** Running training *****\n",
            "[INFO|trainer.py:1722] 2024-01-25 01:52:42,948 >>   Num examples = 72,927\n",
            "[INFO|trainer.py:1723] 2024-01-25 01:52:42,948 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1724] 2024-01-25 01:52:42,948 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1727] 2024-01-25 01:52:42,948 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:1728] 2024-01-25 01:52:42,948 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1729] 2024-01-25 01:52:42,948 >>   Total optimization steps = 54,696\n",
            "[INFO|trainer.py:1730] 2024-01-25 01:52:42,949 >>   Number of trainable parameters = 3,145,728\n",
            "  0% 0/54696 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "  0% 29/54696 [01:18<41:47:43,  2.75s/it]"
          ]
        }
      ],
      "source": [
        "! CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path  Qwen/Qwen-1_8B-Chat\\\n",
        "    --do_train True\\\n",
        "    --dataset haruhi_dialogue_extract \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target c_attn \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --output_dir qwen_1_8-finetuned \\\n",
        "    --overwrite_output_dir \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 100 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9OaXCFQLze_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnCRoYDlBf1oKogyMcqT4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}